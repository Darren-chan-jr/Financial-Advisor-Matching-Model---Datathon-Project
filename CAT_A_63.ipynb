{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darren-chan-jr/Financial-Advisor-Matching-Model---Datathon-Project/blob/main/CAT_A_63.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing libraries that are installed\n",
        "%pip list"
      ],
      "metadata": {
        "id": "T0rQYTRFQMPS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries using pip tool\n",
        "%pip install scikit-learn\n",
        "%pip install pandas\n",
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install seaborn\n",
        "%pip install scipy\n",
        "%pip install pyarrow\n",
        "%pip install shap\n",
        "%pip install lightgbm\n",
        "%pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "l4B5YfNjQm0P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary **libraries**"
      ],
      "metadata": {
        "id": "9PiZ-0lQp29f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyarrow.parquet as pq\n",
        "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, silhouette_score\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import shap\n",
        "import re\n",
        "import ast # To safely convert string representations of lists\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "wipy_3icjcyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Google Drive"
      ],
      "metadata": {
        "id": "p7-lVXKKQqeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hXZ_em5OpwTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "YV4tyabJkABW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Read Agent\n",
        "\n",
        "AGENT_PATH = '/content/drive/MyDrive/data/final_nus_datathon_dataset/nus_agent_info_df.parquet' # To update acording to the path of your data file\n",
        "agent_df = pd.read_parquet(AGENT_PATH, engine='pyarrow')\n",
        "agent_df.shape\n",
        "agent_df.info() # Only agent_age has 12 null values\n",
        "agent_df.head()"
      ],
      "metadata": {
        "id": "3nYPWIzuIKIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Read Policy\n",
        "\n",
        "POLICY_PATH = '/content/drive/MyDrive/data/final_nus_datathon_dataset/nus_policy_info_df.parquet' # To update acording to the path of your data file\n",
        "policy_df = pd.read_parquet(POLICY_PATH, engine='pyarrow')\n",
        "policy_df.shape\n",
        "policy_df.info() # No null values\n",
        "policy_df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E2pxCJfQLx0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Read Client\n",
        "\n",
        "CLIENT_PATH = '/content/drive/MyDrive/data/final_nus_datathon_dataset/nus_client_info_df.parquet' # To update acording to the path of your data file\n",
        "client_df = pd.read_parquet(CLIENT_PATH, engine='pyarrow')\n",
        "client_df.shape\n",
        "client_df.info()\n",
        "client_df.head()\n",
        "# cltdob: 6 null; race_desc_map: 10 null; cltpcode: 156 null; household_size, economic_status, family_size: 343 null each"
      ],
      "metadata": {
        "id": "W2hwZgNxp2Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "L5WNwB5pkIpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n",
        "\n"
      ],
      "metadata": {
        "id": "mK2Wa4NkEeCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clean Agent Data\n",
        "\n",
        "def clean_expertise(entry):\n",
        "    \"\"\"Cleans 'agent_product_expertise' column, ensuring lists are properly formatted.\"\"\"\n",
        "    if isinstance(entry, str):\n",
        "        try:\n",
        "            parsed_list = ast.literal_eval(entry)  # Convert string to list\n",
        "            if isinstance(parsed_list, str):\n",
        "                parsed_list = re.sub(r'(?<=prod_\\d)(?=prod_)', ',', parsed_list)\n",
        "                return parsed_list.split(',')\n",
        "            return parsed_list\n",
        "        except (ValueError, SyntaxError):\n",
        "            return []\n",
        "    elif isinstance(entry, list):\n",
        "        return entry if len(entry) != 1 or not isinstance(entry[0], list) else entry[0]\n",
        "    return []\n",
        "\n",
        "# Apply cleaning\n",
        "agent_df[\"agent_product_expertise\"] = agent_df[\"agent_product_expertise\"].apply(clean_expertise)\n",
        "\n",
        "# Impute agent_age null values with the median age\n",
        "agent_df[\"agent_age\"] = agent_df[\"agent_age\"].fillna(agent_df[\"agent_age\"].median())\n",
        "\n"
      ],
      "metadata": {
        "id": "j2gGcHGgkpqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_df.head()\n",
        "agent_df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oZzq2_NgKmXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clean Policy Data\n",
        "\n",
        "# Standardize product, age and tenure groups\n",
        "def extract_number(pattern, x):\n",
        "    match = re.search(pattern, x) if pd.notnull(x) else None\n",
        "    return int(match.group(1)) if match else None\n",
        "\n",
        "policy_df[\"product_num\"] = policy_df[\"product\"].apply(lambda x: extract_number(r'prod_(\\d+)', x))\n",
        "policy_df[\"product_grp_num\"] = policy_df[\"product_grp\"].apply(lambda x: extract_number(r'PG:(\\d+)', x))\n",
        "policy_df[\"cust_age_group_num\"] = policy_df[\"cust_age_at_purchase_grp\"].apply(lambda x: extract_number(r'AG(\\d+)', x))\n",
        "policy_df[\"cust_tenure_group_num\"] = policy_df[\"cust_tenure_at_purchase_grp\"].apply(lambda x: extract_number(r'TNR(\\d+)', x))\n",
        "\n",
        "# Drop columns\n",
        "policy_df.drop(columns=[\"product\", \"product_grp\", \"cust_age_at_purchase_grp\", \"cust_tenure_at_purchase_grp\", \"flg_main\", \"flg_inforce\", \"flg_cancel\", \"flg_expire\", \"flg_converted\"], inplace=True)\n"
      ],
      "metadata": {
        "id": "OJd4ky1yF4eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_df.head()\n",
        "policy_df.info()"
      ],
      "metadata": {
        "id": "5lBUq8peKugk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clean Client Data\n",
        "\n",
        "# cltdob: 6 null; race_desc_map: 10 null; cltpcode: 156 null; household_size, economic_status, family_size: 343 null each\n",
        "\n",
        "# Impute `cltdob` (Date of Birth) using median age\n",
        "client_df[\"cltdob\"] = pd.to_datetime(client_df[\"cltdob\"], errors='coerce')  # Ensure datetime format\n",
        "median_age = client_df[\"cltdob\"].dropna().dt.year.median()\n",
        "client_df[\"cltdob\"] = client_df[\"cltdob\"].fillna(pd.Timestamp(year=int(median_age), month=1, day=1))\n",
        "\n",
        "#Converting dob to age\n",
        "client_df['clt_Age'] = (pd.Timestamp.today() - client_df['cltdob']).dt.days // 365\n",
        "# Convert Age to integer\n",
        "client_df['clt_Age'] = client_df['clt_Age'].astype(int)\n",
        "\n",
        "# Impute `race_desc_map` (Race) using mode (\"Chinese\")\n",
        "race_mode = client_df[\"race_desc_map\"].mode()[0]\n",
        "client_df[\"race_desc_map\"] = client_df[\"race_desc_map\"].fillna(race_mode)\n",
        "\n",
        "# Note: household_size and family_size values are strangely large. Why?\n",
        "# Impute `household_size`, `economic_status`, `family_size` using mode\n",
        "for col in [\"household_size\", \"economic_status\", \"family_size\"]:\n",
        "    mode_value = client_df[col].mode()[0]\n",
        "    client_df[col] = client_df[col].fillna(mode_value)\n",
        "\n",
        "\n",
        "# Standardize household_size_grp and family_size_grp\n",
        "client_df[\"household_size_grp_num\"] = client_df[\"household_size_grp\"].apply(lambda x: extract_number(r'HH(\\d+)', x))\n",
        "client_df[\"family_size_grp_num\"] = client_df[\"family_size_grp\"].apply(lambda x: extract_number(r'FS(\\d+)', x))\n",
        "\n",
        "# Drop original categorical columns after transformation\n",
        "client_df.drop(columns=[\"household_size_grp\", \"family_size_grp\"], inplace=True)\n",
        "\n",
        "client_df['economic_status'] = client_df['economic_status'].astype('int')\n",
        "client_df['household_size'] = client_df['household_size'].astype('int')\n",
        "client_df['family_size'] = client_df['family_size'].astype('int')\n"
      ],
      "metadata": {
        "id": "vifRWdYuGqAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client_df.head()\n",
        "client_df.info()"
      ],
      "metadata": {
        "id": "jb1nhAMVKxMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "4ZnlcZgSmUgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining Agent, Policy and Client Tables"
      ],
      "metadata": {
        "id": "kfWv4ucpuRYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge policy table with client information based on 'secuityno'\n",
        "policy_client_df = policy_df.merge(client_df, on=\"secuityno\", how=\"left\")\n",
        "# Merge the result with agent information based on 'agntnum'\n",
        "full_data = policy_client_df.merge(agent_df, on=\"agntnum\", how=\"left\")"
      ],
      "metadata": {
        "id": "d0QbjXr2uhTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data.head()\n",
        "full_data.info()"
      ],
      "metadata": {
        "id": "Yvcbq8p7xSl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot Encoding for Nominal data"
      ],
      "metadata": {
        "id": "xGXXAKTzmi_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical columns to encode\n",
        "categorical_cols = [\"agent_gender\", \"agent_marital\", \"cltsex\", \"marryd\", \"race_desc_map\"]\n",
        "\n",
        "# Apply OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_array = encoder.fit_transform(full_data[categorical_cols])\n",
        "\n",
        "# Convert to DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols)).astype(int)\n",
        "\n",
        "# Merge encoded data and drop original categorical columns\n",
        "full_data = pd.concat([full_data.drop(columns=categorical_cols), encoded_df], axis=1)"
      ],
      "metadata": {
        "id": "j81qZ8OsTh9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_data.head())\n",
        "print(full_data.info())"
      ],
      "metadata": {
        "id": "jqiBVqPVZTd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encode Multi-Label Columns\n",
        "mlb = MultiLabelBinarizer()\n",
        "expertise_encoded = mlb.fit_transform(full_data['agent_product_expertise'])\n",
        "\n",
        "# Convert to DataFrame and merge\n",
        "expertise_df = pd.DataFrame(expertise_encoded, columns=mlb.classes_)\n",
        "full_data = pd.concat([full_data.drop(columns=['agent_product_expertise']), expertise_df], axis=1)"
      ],
      "metadata": {
        "id": "MTH6HQa1IkEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Handle Missing Values & Final Checks\n",
        "\n",
        "# Drop rows with remaining NaN values\n",
        "full_data.dropna(inplace=True)\n",
        "\n",
        "# Verify no missing values remain\n",
        "assert full_data.isnull().sum().sum() == 0"
      ],
      "metadata": {
        "id": "FIC9-aHrIy7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display cleaned dataset\n",
        "print(full_data.head())"
      ],
      "metadata": {
        "id": "f8Gz6Bt5JKgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling"
      ],
      "metadata": {
        "id": "Tli9uRl6mzkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Client"
      ],
      "metadata": {
        "id": "2lkGr5azL99K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical features\n",
        "features = ['cltsex_F', 'cltsex_M', 'marryd_D', 'marryd_M', 'marryd_P', 'marryd_S', 'marryd_U', 'marryd_W',  'race_desc_map_Indian', 'race_desc_map_Malay', 'race_desc_map_Others', 'clt_Age', 'household_size', 'economic_status', 'family_size']\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(full_data[features])"
      ],
      "metadata": {
        "id": "3HDUgjNZThM-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training with scikit-learn"
      ],
      "metadata": {
        "id": "4gpvmWc2Vy4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "KUTLBnjo9S1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "\n",
        "# Visualize clusters\n",
        "#sns.pairplot(full_data, hue='Cluster', palette='viridis')\n",
        "#plt.show()\n",
        "\n",
        "# Silhouette Score\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in range(2, 11):  # Silhouette Score requires at least 2 clusters\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(df_scaled)\n",
        "    silhouette_scores.append(silhouette_score(df_scaled, cluster_labels))\n",
        "\n",
        "# Plot Silhouette Scores\n",
        "plt.plot(range(2, 11), silhouette_scores, marker=\"o\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Score for Optimal k\")\n",
        "plt.show()\n",
        "\n",
        "# Fit K-Means with chosen k\n",
        "optimal_k = 6\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "full_data['Cluster'] = kmeans.fit_predict(df_scaled)\n"
      ],
      "metadata": {
        "id": "5CmGDHegYsYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_data.info())"
      ],
      "metadata": {
        "id": "nNnZYKR4CzBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by cluster and analyze feature means\n",
        "cluster_summary = full_data.groupby(\"Cluster\").mean(numeric_only=True)\n",
        "print(cluster_summary)\n",
        "\n",
        "numeric_features = ['household_size', 'economic_status', 'family_size', 'clt_Age']\n",
        "\n",
        "for feature in numeric_features:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.boxplot(x=\"Cluster\", y=feature, data=full_data)\n",
        "    plt.title(f\"{feature} by Cluster\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "72N81msqGycZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['cltsex_F', 'cltsex_M', 'marryd_D', 'marryd_M', 'marryd_P', 'marryd_S', 'marryd_U', 'marryd_W',  'race_desc_map_Indian', 'race_desc_map_Malay', 'race_desc_map_Others']\n",
        "\n",
        "\n",
        "# Compute the mean of each dummy variable per cluster\n",
        "cluster_composition = full_data.groupby(\"Cluster\")[categorical_features].mean()\n",
        "\n",
        "# Plot stacked bar chart\n",
        "cluster_composition.T.plot(kind=\"bar\", stacked=True, figsize=(10, 6), colormap=\"viridis\")\n",
        "plt.title(\"Cluster Composition Across Categorical Variables\")\n",
        "plt.ylabel(\"Proportion of Members in Each Cluster\")\n",
        "plt.legend(title=\"Cluster\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i2dM8HInrPIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=\"Cluster\", hue=feature, data=full_data)\n",
        "    plt.title(f\"Distribution of {feature} in Each Cluster\")\n",
        "    plt.legend(title=feature)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "LbPTWAKQszxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce data to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Add cluster labels to the reduced data\n",
        "df_pca = pd.DataFrame(df_pca, columns=[\"PC1\", \"PC2\"])\n",
        "df_pca[\"Cluster\"] = full_data[\"Cluster\"]\n",
        "\n",
        "# Plot the clusters\n",
        "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=df_pca, palette=\"viridis\")\n",
        "plt.title(\"K-Means Clustering (2D PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tMfwy9wKuHNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering"
      ],
      "metadata": {
        "id": "d8pzry2Yzphu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute the linkage matrix (Ward's method minimizes variance within clusters)\n",
        "linkage_matrix = sch.linkage(df_scaled, method='ward')\n",
        "\n",
        "# Plot the Dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sch.dendrogram(linkage_matrix, truncate_mode='level', p=10)  # Show only top 10 levels\n",
        "plt.xlabel(\"Data Points\")\n",
        "plt.ylabel(\"Distance\")\n",
        "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FPwH4qIfvwPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dont run this!!!!! (Very time consuming)\n",
        "\n",
        "silhouette_scores = []\n",
        "\n",
        "# Test different cluster numbers\n",
        "for k in range(2, 11):  # Hierarchical clustering must have at least 2 clusters\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "    cluster_labels = hierarchical.fit_predict(df_scaled)\n",
        "\n",
        "    score = silhouette_score(df_scaled, cluster_labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot Silhouette Scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(2, 11), silhouette_scores, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.title(\"Silhouette Score for Different Cluster Counts (Hierarchical Clustering)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yK_w6to105EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agglomerative = AgglomerativeClustering(n_clusters=6)\n",
        "full_data[\"Cluster_Hierarchical\"] = agglomerative.fit_predict(df_scaled)\n"
      ],
      "metadata": {
        "id": "u2Azr9ebvLlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_hc = silhouette_score(df_scaled, full_data[\"Cluster_Hierarchical\"])\n",
        "print(f\"Silhouette Score (Hierarchical): {silhouette_hc}\")"
      ],
      "metadata": {
        "id": "IcTAd258xByb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping of numerical cluster labels to descriptive names\n",
        "cluster_mapping = {\n",
        "    0: \"Affluent Middle Aged Chinese Families with Mixed Marital Status\",\n",
        "    1: \"Middle-Aged Married Chinese and Multiracial Families\",\n",
        "    2: \"Middle-Aged Divorced Individuals with Mixed Ethnicity and Small Families\",\n",
        "    3: \"Middle-Aged Married Individuals with Mixed Ethnicity and Large Families\",\n",
        "    4: \"Young Separated Chinese Individuals with Small Families\",\n",
        "    5: \"Elderly Widowed Chinese Women with Low Economic Status\"\n",
        "}\n",
        "\n",
        "# Apply the mapping to the cluster labels\n",
        "full_data[\"Cluster_Hierarchical\"] = full_data[\"Cluster_Hierarchical\"].replace(cluster_mapping)\n",
        "\n",
        "# Display the updated DataFrame with new labels\n",
        "print(full_data[[\"Cluster_Hierarchical\"]].head())\n"
      ],
      "metadata": {
        "id": "MjEvhgZ_AVfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Analysis of Cluster Characteristics"
      ],
      "metadata": {
        "id": "lqs76Z0bAWcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert specific columns to numeric if they are expected to be numeric\n",
        "for col in full_data.columns:\n",
        "    if full_data[col].dtype == 'object':  # Check if the column is of type 'object'\n",
        "        try:\n",
        "            full_data[col] = pd.to_numeric(full_data[col], errors='ignore') # Try converting to numeric; ignore errors if conversion fails\n",
        "        except (ValueError, TypeError):\n",
        "            pass  # Handle potential errors during conversion, e.g., if the column contains non-numeric strings\n",
        "\n",
        "# Calculate cluster summary using the modified DataFrame\n",
        "cluster_summary = full_data.groupby(\"Cluster_Hierarchical\").mean(numeric_only=True)  #Explicitly tell pandas to only consider numeric columns for the mean calculation.\n",
        "print(cluster_summary)\n"
      ],
      "metadata": {
        "id": "ldWjqInKAcx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = ['household_size', 'economic_status', 'family_size', 'clt_Age']\n",
        "\n",
        "for feature in numeric_features:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.boxplot(x=\"Cluster_Hierarchical\", y=feature, data=full_data)\n",
        "    plt.title(f\"{feature} by Cluster\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4K3p6YzNA4qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['cltsex_F', 'cltsex_M', 'marryd_D', 'marryd_M', 'marryd_P', 'marryd_S', 'marryd_U', 'marryd_W',  'race_desc_map_Indian', 'race_desc_map_Malay', 'race_desc_map_Others']\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=\"Cluster_Hierarchical\", hue=feature, data=full_data)\n",
        "    plt.title(f\"Distribution of {feature} in Each Cluster\")\n",
        "    plt.legend(title=feature)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "I0Ilh7KuBLbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identifying Agent Feature Importance for each cluster\n",
        "\n"
      ],
      "metadata": {
        "id": "UKQcTsCjEJqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install pandas\n",
        "#%pip install scikit-learn\n",
        "%pip install imbalanced-learn\n",
        "%pip install lightgbm\n",
        "\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define features (independent variables)\n",
        "agent_features = [\n",
        "    \"agent_age\", \"agent_tenure\", \"cnt_converted\", \"annual_premium_cnvrt\",\n",
        "    \"pct_lapsed\", \"pct_cancel\", \"pct_inforce\",\n",
        "    \"pct_prod_0_cnvrt\", \"pct_prod_1_cnvrt\", \"pct_prod_2_cnvrt\", \"pct_prod_3_cnvrt\",\n",
        "    \"pct_prod_4_cnvrt\", \"pct_prod_5_cnvrt\", \"pct_prod_6_cnvrt\", \"pct_prod_7_cnvrt\",\n",
        "    \"pct_prod_8_cnvrt\", \"pct_prod_9_cnvrt\",\n",
        "    \"pct_SX0_unknown\", \"pct_SX1_male\", \"pct_SX2_female\",\n",
        "    \"pct_AG01_lt20\", \"pct_AG02_20to24\", \"pct_AG03_25to29\", \"pct_AG04_30to34\",\n",
        "    \"pct_AG05_35to39\", \"pct_AG06_40to44\", \"pct_AG07_45to49\", \"pct_AG08_50to54\",\n",
        "    \"pct_AG09_55to59\", \"pct_AG10_60up\",\n",
        "    \"agent_gender_M\", \"agent_gender_U\",\n",
        "    \"agent_marital_M\", \"agent_marital_S\", \"agent_marital_U\", \"agent_marital_W\"\n",
        "]\n",
        "\n",
        "X = full_data[agent_features]  # Features\n",
        "y = full_data[\"Cluster\"]  # Target (customer cluster)\n",
        "\n",
        "# Dictionary to store trained models for each cluster\n",
        "models_per_cluster = {}\n",
        "\n",
        "# Loop through each unique cluster and train a separate model\n",
        "for cluster in y.unique():\n",
        "    print(f\"Training model for Cluster {cluster}...\")\n",
        "\n",
        "    # Select only agents assigned to this cluster\n",
        "    X_cluster = X[y == cluster]\n",
        "    y_cluster = y[y == cluster]\n",
        "\n",
        "\n",
        "\n",
        "    # Split into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_cluster, y_cluster, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train LightGBM model for this cluster\n",
        "    model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store model in dictionary\n",
        "    models_per_cluster[cluster] = model\n",
        "\n",
        "    # Evaluate model\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Cluster {cluster} Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"Cluster {cluster} Classification Report:\\n{classification_report(y_test, y_pred)}\\n\")\n",
        "\n",
        "# Display trained models\n",
        "print(\"Models trained for clusters:\", models_per_cluster.keys())"
      ],
      "metadata": {
        "id": "ie3NWnDAbLq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Gini Importance\n",
        "# feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "# print(feature_importance)\n",
        "\n",
        "feature_names = X.columns\n",
        "importances = rf_model.feature_importances_\n",
        "feature_impt = pd.DataFrame({'Feature': feature_names, 'Gini Importance': importances}).sort_values('Gini Importance', ascending=False)\n",
        "print(feature_impt)\n"
      ],
      "metadata": {
        "id": "21Ij8MLtOHN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 15 features based on Gini Importance\n",
        "top_n = 15\n",
        "top_features_df = feature_impt.nlargest(top_n, 'Gini Importance')\n",
        "\n",
        "# Create a horizontal bar plot for the top 15 features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_features_df[\"Feature\"], top_features_df[\"Gini Importance\"], color='skyblue')\n",
        "\n",
        "# Formatting\n",
        "plt.xlabel('Gini Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title(f'Top {top_n} Most Important Features - Gini Importance')\n",
        "plt.gca().invert_yaxis()  # Flip Y-axis for readability\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YrLioRxPqrEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SHAP Values\n",
        "\n",
        "# Create SHAP Explainer\n",
        "explainer = shap.TreeExplainer(rf_model)\n",
        "\n",
        "# Compute SHAP values\n",
        "shap_values = explainer.shap_values(X_test)  # Get SHAP values for test data\n",
        "\n",
        "# Check the shape of SHAP values\n",
        "print(np.array(shap_values).shape)  # Should be (num_classes, num_samples, num_features)\n",
        "\n",
        "# Summary Plot\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "# Feature Importance Bar Chart\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "fZXBDnYJdZWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hyperparameter Tuning\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_dist = {\n",
        "    \"n_estimators\": [100, 200, 500, 1000],\n",
        "    \"max_depth\": [10, 20, 30, None],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None]\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "rf_random = RandomizedSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,  # Number of different combinations to try\n",
        "    cv=3,  # 3-fold cross-validation\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_random.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", rf_random.best_params_)\n"
      ],
      "metadata": {
        "id": "eSLNa2FzkR9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with best parameters\n",
        "final_model = RandomForestClassifier(rf_random.best_params_, random_state=42)\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_final = final_model.predict(X_test)\n",
        "\n",
        "# Final Model Accuracy\n",
        "print(\"Final Model Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
        "print(classification_report(y_test, y_pred_final))\n"
      ],
      "metadata": {
        "id": "S71V5A6jkqUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "2kDY94rlCA6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge client clusters with agent features\n",
        "recommendation_data = full_data[[\"Cluster_Hierarchical\", \"agntnum\"] + top_agent_features]\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = recommendation_data.drop(columns=[\"agntnum\"])  # Features: client cluster + agent features\n",
        "y = recommendation_data[\"agntnum\"]  # Target: best agent\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest Model\n",
        "rf_optimized = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_optimized.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_optimized.predict(X_test)\n",
        "\n",
        "# Evaluate Performance\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Optimized Model Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "71H__UA2rQaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assign Customers to an Agent Cluster\n",
        "\n",
        "# Select agent performance features\n",
        "agent_performance_features = [\"cnt_converted\", \"annual_premium_cnvrt\", \"pct_lapsed\", \"pct_cancel\", \"pct_inforce\"]\n",
        "\n",
        "# Apply K-Means to group agents into clusters\n",
        "kmeans_agents = KMeans(n_clusters=4, random_state=42)\n",
        "full_data[\"agent_cluster\"] = kmeans_agents.fit_predict(full_data[agent_performance_features])\n",
        "\n",
        "# Map customers to agent clusters\n",
        "full_data[\"recommended_agent_cluster\"] = full_data[\"Cluster_Hierarchical\"].map(\n",
        "    lambda x: kmeans_agents.predict(\n",
        "        full_data[full_data[\"Cluster_Hierarchical\"] == x][agent_performance_features].mean().values.reshape(1, -1)\n",
        "    )[0]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "8s7RMotUvj-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Group by Cluster and Agent\n",
        "agent_cluster_metrics = full_data.groupby([\"Cluster_Hierarchical\", \"agntnum\"]).agg(\n",
        "    cnt_converted=(\"cnt_converted\", \"sum\"),  # Total policies converted\n",
        "    pct_lapsed=(\"pct_lapsed\", \"mean\"),      # Average lapse percentage\n",
        "    pct_cancel=(\"pct_cancel\", \"mean\"),      # Average cancel percentage\n",
        "    pct_inforce=(\"pct_inforce\", \"mean\")     # Average in-force percentage\n",
        ").reset_index()\n",
        "\n",
        "# Step 2: Calculate Normalized Success Rate\n",
        "# Adjust for the balance between in-force, lapsed, and canceled policies\n",
        "agent_cluster_metrics[\"normalized_success_rate\"] = (\n",
        "    agent_cluster_metrics[\"pct_inforce\"]\n",
        "    - agent_cluster_metrics[\"pct_lapsed\"]\n",
        "    - agent_cluster_metrics[\"pct_cancel\"]\n",
        ")\n",
        "\n",
        "# Normalize to a scale (0 to 1)\n",
        "agent_cluster_metrics[\"normalized_success_rate\"] = (\n",
        "    agent_cluster_metrics[\"normalized_success_rate\"]\n",
        "    - agent_cluster_metrics[\"normalized_success_rate\"].min()\n",
        ") / (\n",
        "    agent_cluster_metrics[\"normalized_success_rate\"].max()\n",
        "    - agent_cluster_metrics[\"normalized_success_rate\"].min()\n",
        ")\n",
        "\n",
        "# Step 3: Combine Metrics for Final Ranking\n",
        "agent_cluster_metrics[\"combined_score\"] = (\n",
        "    agent_cluster_metrics[\"cnt_converted\"] * 0.6  # Weight for count converted\n",
        "    + agent_cluster_metrics[\"normalized_success_rate\"] * 0.4  # Weight for success rate\n",
        ")\n",
        "\n",
        "# Rank agents within each cluster\n",
        "agent_cluster_metrics[\"rank\"] = agent_cluster_metrics.groupby(\"Cluster_Hierarchical\")[\n",
        "    \"combined_score\"\n",
        "].rank(ascending=False)\n",
        "\n",
        "# Step 4: Output the Rankings\n",
        "for cluster_name, cluster_data in agent_cluster_metrics.groupby(\"Cluster_Hierarchical\"):\n",
        "    print(f\"Top agents for cluster '{cluster_name}':\")\n",
        "    print(cluster_data.sort_values(\"rank\").head(5))  # Top 5 agents per cluster\n"
      ],
      "metadata": {
        "id": "zRydLqlkrSYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Combine agent features with client segment data\n",
        "# Assuming `agent_df` contains agent features and `full_data` includes client clusters\n",
        "numeric_columns = full_data.select_dtypes(include=[\"number\"]).columns\n",
        "\n",
        "# Step 2: Group by the Cluster_Hierarchical and aggregate numeric columns\n",
        "agent_data = full_data.groupby(\"Cluster_Hierarchical\")[numeric_columns].mean().reset_index()\n",
        "\n",
        "agent_data = agent_df.copy()\n",
        "\n",
        "# Merge agent data with cluster-level data\n",
        "matching_data = agent_data.merge(agent_cluster_data, how=\"cross\")\n",
        "\n",
        "# Step 2: Define target and features\n",
        "# Use `cnt_converted` or a similar proxy metric as the target\n",
        "matching_data[\"success_metric\"] = (\n",
        "    matching_data[\"pct_inforce\"] - (matching_data[\"pct_cancel\"] + matching_data[\"pct_lapsed\"])\n",
        ")\n",
        "\n",
        "# Define features for prediction\n",
        "agent_features = [\n",
        "    \"agent_age\", \"agent_gender\", \"agent_marital\", \"agent_tenure\",\n",
        "    \"pct_sx0_unknown\", \"pct_sx1_male\", \"pct_sx2_female\",\n",
        "    \"pct_prod_0_cnvrt\", \"pct_prod_1_cnvrt\", \"pct_prod_2_cnvrt\",  # Add all pct_prod_* columns\n",
        "    \"pct_ag01_lt20\", \"pct_ag02_20to24\", \"pct_ag03_25to29\",  # Add all pct_ag* columns\n",
        "]\n",
        "\n",
        "client_features = [\"household_size\", \"economic_status\", \"family_size\", \"clt_Age\"]\n",
        "features = agent_features + client_features\n",
        "\n",
        "# Step 3: Train a model to match agents to clusters\n",
        "X = matching_data[features]\n",
        "y = (matching_data[\"success_metric\"] > 0).astype(int)  # Binary classification: Success or not\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 5: Predict and rank agents for each cluster\n",
        "matching_data[\"predicted_success\"] = model.predict_proba(X)[:, 1]\n",
        "\n",
        "# Rank agents by predicted success for each cluster\n",
        "ranked_agents = (\n",
        "    matching_data.groupby(\"Cluster_Hierarchical\")\n",
        "    .apply(lambda x: x.sort_values(\"predicted_success\", ascending=False))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Display top agents for each cluster\n",
        "print(ranked_agents[[\"Cluster_Hierarchical\", \"agntnum\", \"predicted_success\"]].head())"
      ],
      "metadata": {
        "id": "grPTXU_otIxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary target variable\n",
        "full_data['agent_conversion_success'] = (full_data['cnt_converted'] > 0).astype(int)\n",
        "\n",
        "# Step 1: Ensure 'agent_conversion_success' is the target variable\n",
        "full_data['agent_conversion_success'] = full_data['agent_conversion_success'].astype(int)\n",
        "\n",
        "# Step 2: Identify feature columns (excluding cluster and target)\n",
        "feature_cols = full_data.select_dtypes(include=np.number).columns.tolist()\n",
        "feature_cols = [col for col in feature_cols if col not in [\"agent_conversion_success\", \"Cluster\", \"agntnum\", \"Cluster_Hierarchical\", 'Cluster', 'Cluster_Hierarchical']]  # Remove target and cluster columns\n",
        "\n",
        "# Step 3: Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "full_data[feature_cols] = scaler.fit_transform(full_data[feature_cols])\n",
        "\n",
        "# Step 4: Train a logistic regression model for each cluster\n",
        "cluster_models = {}\n",
        "for cluster in full_data[\"Cluster_Hierarchical\"].unique():\n",
        "    print(f\"Training Logistic Regression for Cluster: {cluster}\")\n",
        "\n",
        "    # Filter data for this cluster\n",
        "    cluster_data = full_data[full_data[\"Cluster_Hierarchical\"] == cluster]\n",
        "\n",
        "    # Split into training & testing\n",
        "    X_train, X_test, y_train, y_test = train_test_split(cluster_data[feature_cols],\n",
        "                                                        cluster_data[\"agent_conversion_success\"],\n",
        "                                                        test_size=0.2, random_state=42)\n",
        "\n",
        "    # Check if y_test has exactly two classes\n",
        "    if len(np.unique(y_test)) != 2:\n",
        "        print(f\"Skipping cluster {cluster} because y_test has only one class: {np.unique(y_test)}\")\n",
        "        continue\n",
        "\n",
        "    # Train logistic regression\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Store trained model\n",
        "    cluster_models[cluster] = model\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
        "\n",
        "# Step 5: Predict best agent for a new client\n",
        "def predict_best_agent(client_features, client_cluster, agent_data):\n",
        "    \"\"\"\n",
        "    Given a clientâ€™s features and cluster, predict the agent with the highest probability of conversion.\n",
        "\n",
        "    :param client_features: DataFrame with client features (matching feature_cols)\n",
        "    :param client_cluster: The client's assigned cluster\n",
        "    :param agent_data: DataFrame with all agent features (matching feature_cols)\n",
        "    :return: Best agent's ID and probability\n",
        "    \"\"\"\n",
        "    model = cluster_models.get(client_cluster)\n",
        "    if not model:\n",
        "        raise ValueError(f\"No model found for cluster: {client_cluster}\")\n",
        "\n",
        "    # Predict probabilities for each agent\n",
        "    agent_probabilities = model.predict_proba(agent_data[feature_cols])[:, 1]\n",
        "\n",
        "    # Find the best agent\n",
        "    best_agent_idx = np.argmax(agent_probabilities)\n",
        "    best_agent_id = agent_data.iloc[best_agent_idx][\"agntnum\"]\n",
        "    best_agent_prob = agent_probabilities[best_agent_idx]\n",
        "\n",
        "    return best_agent_id, best_agent_prob\n",
        "\n"
      ],
      "metadata": {
        "id": "RyaTuTLj_Oeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}